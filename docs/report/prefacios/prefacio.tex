\thispagestyle{empty}

\begin{center}

{\large\bfseries Deep G. Prop. \\ Optimización de parámetros de perceptrones
multicapa}\\

\end{center}

\begin{center}

    Luis Liñán Villafranca \\

\end{center}

\vspace{0.7cm}

\vspace{0.5cm}
\noindent{\textbf{Palabras clave}:
\textit{Software libre},
\textit{Redes neuronales},
\textit{Algoritmos genéticos},
\textit{Optimización de parámetros},
\textit{Clasificación}}
\vspace{0.7cm}

\noindent{\textbf{Resumen}}\\

Existe una cuestión que concierne a los modelos de clasificación supervisada:
encontrar la configuración de parámetros adecuada al problema al que se aplica.
Los perceptrones multicapa (MLP) con propagación hacia atrás (backpropagation)
tienen multitud valores de entrada, como el número de capas ocultas, el número
de neuronas (o perceptrones) en cada capa, los pesos entre las neuronas, la
función de activación de cada neurona... Lo cual hace que su correcta
inicialización suponga una tarea difícil y costosa. Los algoritmos genéticos
(GA) son una buena opción para solventar ésta, ya que realizando las
operaciones características de éstos (que simulan el proceso de selección
natural) se puede obtener una solución optimizada de parámetros que sirvan como
punto de partida para los perceptrones multicapa.

\cleardoublepage

\begin{center}

    {\large\bfseries Deep G. Prop. \\ Multilayer perceptrons parameters
    optimization}\\

\end{center}

\begin{center}

    Luis Liñán Villafranca\\

\end{center}

\vspace{0.5cm}
\noindent{\textbf{Keywords}:
\textit{free software}
\textit{neural networks},
\textit{genetic algorithms},
\textit{parameters optimization},
\textit{classification}}
\vspace{0.7cm}

\noindent{\textbf{Abstract}}\\

Supervised learning (categorized inside machine learning) deals with classified
data, in which each example comes associated to a label. The goal of these
problems is to train a model to be able to correctly classify a new example
without knowing the associated label. One of the big problems of supervised
classification is the ability to find the appropriate parameters configuration
for a particular dataset and model which enables the correct classification of
most of the examples. Multilayer perceptrons (MLP) have a huge variety of
initial parameters to configure the neural network with such as the number of
hidden layers, the number of neurons for each layer, the connection weights
between neurons, the activation function for each neuron... They also have
other parameters associated to weights training when using backpropagation as
learning method such as the optimizer or the loss function. The big quantity of
these aforementioned parameters makes the search a difficult and expensive
undertaking. Among the big variety of methods and algorithms present out there,
the Genetic algorithms (GA) are a great choice. Making use of its common
operations mimicking natural selection flow (selection, crossover, mutation and
replacement) to settle this task, may obtain optimized initial parameters for
the multilayer perceptrons future application.


\cleardoublepage

\thispagestyle{empty}

\noindent\rule[-1ex]{\textwidth}{2pt}\\[4.5ex]

D. \textbf{Juan Julián Merelo Guervós}, Profesor(a) del departamento de
Arquitectura y Tecnología de Computadores de la Universidad de Granada

\vspace{0.5cm}

\textbf{Informo:}

\vspace{0.5cm}

Que el presente trabajo, titulado \textit{\textbf{Deep G. Prop.}}, ha sido
realizado bajo mi supervisión por \textbf{Luis Liñán Villafranca}, y autorizo
la defensa de dicho trabajo ante el tribunal que corresponda.

\vspace{0.5cm}

Y para que conste, expiden y firman el presente informe en Granada a Julio de
2020.

\vspace{1cm}

\textbf{El/la director(a)/es:}

\vspace{5cm}

\noindent\textbf{Juan Julián Merelo Guervós}

\chapter*{Agradecimientos}

Primero quiero agradecer a mi tutor del trabajo de fin de grado, Juan Julián
Merelo Guervós por el apoyo y ánimo recibido durante el transcurso del
proyecto.

Segundo y no menos importante a mis padres, Elena y Luis, por su gran paciencia
y por darme toda la motivación necesaria (y más) para la realización de este
trabajo.

Por último a mis amigos que me han acompañado en esta gran experiencia
ayudándome en lo que podían y dándome ánimos.
