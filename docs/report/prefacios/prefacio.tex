\thispagestyle{empty}

\begin{center}

{\large\bfseries Deep G. Prop. \\ Optimización de parámetros de perceptrones
multicapa}\\

\end{center}

\begin{center}

    Luis Liñán Villafranca \\

\end{center}

\vspace{0.7cm}

\vspace{0.5cm}
\noindent{\textbf{Palabras clave}:
\textit{Software libre},
\textit{Redes neuronales},
\textit{Algoritmos genéticos},
\textit{Optimización de prarámetros},
\textit{Clasificación}}
\vspace{0.7cm}

\noindent{\textbf{Resumen}}\\

Existe una cuestión que concierne a los modelos de clasificación supervisada:
encontrar la configuración de parámetros adecuada al problema al que se aplica.
Los perceptrones multicapa (MLP) con propagación hacia atrás (backpropagation)
tienen multitud valores de entrada, como el número de capas ocultas, el número
de neuronas (o perceptrones) en cada capa, los pesos entre las neuronas, la
función de activación de cada neurona... Lo cual hace que su correcta
inicialización suponga una tarea difícil y costosa. Los algoritmos genéticos
(GA) son una buena opción para solventar ésta, ya que realizando las
operaciones características de éstos (que simulan el proceso de selección
natural) se puede obtener una solución optimizada de parámetros que sirvan como
punto de partida para los perceptrones multicapa.

\cleardoublepage

\begin{center}

    {\large\bfseries Deep G. Prop. \\ Multilayer perceptrons parameters
    optimization}\\

\end{center}

\begin{center}

    Luis Liñán Villafranca\\

\end{center}

\vspace{0.5cm}
\noindent{\textbf{Keywords}:
\textit{free software}
\textit{neural networks},
\textit{genetic algorithms},
\textit{parameters optimization},
\textit{classification}}
\vspace{0.7cm}

\noindent{\textbf{Abstract}}\\

One of the big problems of supervised classification is the ability to find the
appropriate parameters configuration for a particular problem. Multilayer
perceptrons (MLP) with backpropagation have a huge variety of input parameters
such as the number of hidden layers, the number of neurons for each layer, the
connection weights between neurons, the activation function for each neuron...
Which make this a difficult and expensive undertaking. Genetic algorithms (GA)
are a good choice to settle this task, because using their characteristic
operations (mimicking natural selection flow) may obtain optimized input
parameters for the multilayer perceptrons.

\cleardoublepage

\thispagestyle{empty}

\noindent\rule[-1ex]{\textwidth}{2pt}\\[4.5ex]

D. \textbf{Juan Julián Merelo Guervós}, Profesor(a) del ...

\vspace{0.5cm}

\textbf{Informo:}

\vspace{0.5cm}

Que el presente trabajo, titulado \textit{\textbf{Deep G. Prop.}}, ha sido
realizado bajo mi supervisión por \textbf{Luis Liñán Villafranca}, y autorizo
la defensa de dicho trabajo ante el tribunal que corresponda.

\vspace{0.5cm}

Y para que conste, expiden y firman el presente informe en Granada a Junio de
2020.

\vspace{1cm}

\textbf{El/la director(a)/es:}

\vspace{5cm}

\noindent\textbf{Juan Julián Merelo Guervós}

\chapter*{Agradecimientos}




