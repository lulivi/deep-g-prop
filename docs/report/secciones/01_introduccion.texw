\chapter{Introducción}

La \textit{clasificación supervisada} es una parte de la \textit{IA}
(\textit{inteligencia artificial}) que busca clasificar ejemplos en diferentes
grupos a partir de sus características. Los \textit{clasificadores automáticos}
son \textit{metaheurísticas} que mediante \textit{aprendizaje automático} y, a
partir de las características de un ejemplo, lo etiquetan con el subgrupo que
le corresponde. La configuración de los \textit{clasificadores automáticos}
(como las \textit{redes neuronales}) es una tarea complicada y manual, lo cual
hace que se tenga que dedicar demasiado tiempo a la optimización de estos
parámetros.

El \textit{aprendizaje automático} (\textit{ML} por sus siglas en inglés:
\textit{machine learning}) es la capacidad que tiene un sistema de
\textit{inteligencia artificial} para crear un conocimiento propio a partir de
datos en bruto\cite[p.~2]{goodfellow_bengio_courville_2016}. Un subgrupo del
\textit{aprendizaje automático}, el \textit{aprendizaje profundo} (\textit{deep
learning}), estudia problemas en los que algoritmos simples de \textit{ML} no
tienen éxito, como reconocimiento de voz o reconocimiento de
imágenes\cite[p.~15]{goodfellow_bengio_courville_2016}. En este subgrupo se
encuentran las \textit{redes neuronales artificiales} (\textit{ANN} en inglés,
\textit{artificial neural network}) y más en concreto el llamado
\textit{perceptrón multicapa} (\textit{MLP} en inglés, \textit{multilayer
perceptron}).

Otro de los subgrupos del \textit{aprendizaje automático} es el de los
\textit{algoritmos evolutivos} (\textit{evolutionary algorithms} en inglés),
inspirados en los mecanismos de la evolución biológica. Esta serie de
algoritmos (como los \textit{algoritmos genéticos}) tienen etapas comunes, como
la inicialización de la población, la evaluación y selección, y la reproducción
y variación\cite{evolutionary_computation}.

\section{Objetivos}

A parte de los objetivos básicos del proyecto como elegir frameworks para
programar tanto \textit{perceptrones multicapa} como de \textit{algoritmos
genéticos}, se pretende conocer si es posible encontrar configuraciones óptimas
de parámetros para los \textit{perceptrones multicapa}, como en el artículo en
el que se basa el trabajo (G-Prop\cite{g_prop}) pero con varias capas ocultas.

\begin{itemize}

\item Comprobar si se pueden conseguir configuraciones óptimas de \textit{MLP}.

\item Evitar sobreajuste\footnote{Se refiere al entrenamiento excesivo con un
conjunto de datos, dando lugar a muy buenos resultados para éste conjunto pero
erróneos para distintos} en este tipo de configuraciones.

\end{itemize}

