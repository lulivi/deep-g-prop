\chapter{Implementación}

En este capítulo se explicará detalladamente la implementación en Python
seguida para llegar a las conclusiones obtenidas.

El código del trabajo está localizado en el directorio \texttt{src} situado en
la carpeta raíz del repositorio. El \textit{script} llamado
\texttt{deep\_g\_prop} es el punto de entrada del programa.

\section{Herramienta de línea de comandos}

Por línea de comandos se le indica la configuración que se quiere utilizar en
el algoritmo genético. Para información sobre las opciones del programa, se
puede ejecutar lo siguiente:

\begin{verbatim}
python src/deep-g-prop.py --help
\end{verbatim}

Lo cual mostraría por pantalla algo similar a:

{\footnotesize
\begin{verbatim}
Usage: deep_g_prop.py [OPTIONS]

  Run a genetic algorithm with the chosen settings.

Options:
  -d, --dataset-name TEXT         name of the proben1 partition located in
                                  src/datasets/. Default: 'cancer1'

  -ip, --init-pop INTEGER         number of individuals for the first
                                  population. Default: '20'.

  -mg, --max-gen INTEGER          maximun number of generations. Default:
                                  '10'.

  -nr, --neurons-range <INTEGER INTEGER>...
                                  neurons number range for each hidden layer.
                                  Default: '(2, 20)'.

  -lr, --layers-range <INTEGER INTEGER>...
                                  hidden layers number range. Default: '(1,
                                  3)'.

  -cx, --cx-prob FLOAT            probability for two individuals to mate.
                                  Default: '0.5'.

  -b, --mut-bias FLOAT            probability to mutate each individual bias
                                  gene. Default: '0.2'.

  -w, --mut-weights FLOAT         probability to mutate each individual weight
                                  gene. Default: '0.75'.

  -n, --mut-neurons FLOAT         probability to add/remove the last neuron of
                                  a random layer for an individual. Default:
                                  '0.3'.

  -l, --mut-layers FLOAT          probability to add/remove the last layer
                                  from an individual. Default: '0.3'.

  -c, --const-hidden              whether to apply crossover and mutation
                                  operators to the hidden layers. Default:
                                  'False'.

  -v, --verbosity [critical|info|debug]
                                  stream handler verbosity level.
  -s, --seed INTEGER              stream handler verbosity level.
  --help                          Show this message and exit.
  
\end{verbatim}
}

Esta línea de comandos se ha construido usando el módulo
\textit{Click}\cite{py-click}. En ella se diferencian principalmente 10
opciones:

\begin{itemize}

    \item \code{--dataset-name} - nombre de la partición de datos a utilizar.

    \item \code{--init-pop} - tamaño de la población de individuos inicial.
    
    \item \code{--max-gen} - número máximo de generaciones que se ejecutará el
    algoritmo.

    \item \code{--neurons-range} - límites máximo y mínimo para calcular
    aleatoriamente el número de neuronas para cada capa de cada individuo.

    \item \code{--layers-range} - límites máximo y mínimo para calcular
    aleatoriamente el número de capas de cada individuo.
    
    \item \code{--cx-prob} - probabilidad que dos individuos se crucen entre
    sí.

    \item \code{--mut-bias} - probabilidad que un individuo
    sufra modificaciones en cada gen de los conjuntos de sesgos.

    \item \code{--mut-weights} - probabilidad que un individuo
    sufra modificaciones en cada gen de los conjuntos de pesos.

    \item \code{--mut-neurons} - probabilidad que cada individuo de la
    población sufra un añadido / extracción de neurona en una capa aleatoria.

    \item \code{--mut-layers} - probabilidad que cada individuo de la
    población sufra un añadido / extracción de última capa del modelo.

    \item \code{--const-hidden} - \textit{flag} que define si los individuos de
    la población tendrán las capas ocultas fijas, es decir, que no van a ser
    modificadas ni por mutaciones, ni cruces entre individuos.

    \item \code{--seed} - semilla a utilizar por los generadores de números
    aleatorios (el módulo \textit{random} y \textit{numpy.random}). Por defecto
    utiliza la definida en \texttt{src/common.py}.

\end{itemize}

También existe el comando \code{--verbosity} que define la verbosidad de la
terminal (a parte, se guardará toda la información de salida del algoritmo en
un archivo localizado en la carpeta \texttt{src/logs}) y \code{--help} que
muestra el texto de ayuda como se comentó antes.

\section{Utilidades}

Dentro de la carpeta donde se encuentra el código \texttt{src} podemos ver un
módulo llamado \textit{utils.py}. En este módulo se encuentran varias funciones
usadas multiples veces de propósito general. Entre ellas se encuentran:

\begin{itemize}

  \item \code{--read_proben1_partition} - como su propio nombre indica, carga
  los datos de una de las particiones de Proben1 y los devuelve como un
  conjunto de arrays multidimensionales de \textit{Numpy}\cite{py-numpy} para
  su cómoda utilización.

  \item \code{--read_all_proben1_partitions} - obtiene todas las particiones de
  un mismo problema. Por ejemplo
  ``cancer'' obtendría las   particiones ``cancer1'',
  ``cancer2'' y ``cancer3''.

  \item \code{--print_table} - muestra una tabla de python de forma limpia y
  legible.

  \item \code{--print_data_summary} - dada una partición de Proben1 muestra
  información util sobre ella, como el número de clases, la distribución de
  ejemplos entre ellas, etc.

\end{itemize}

También se puede localizar el módulo \textit{dataset\_to\_proben1.py}, que ha
sido utilizado para obtener particiones de tipo Proben1 con datasets completos
como \textit{Spambase}. Es una línea de comandos muy sencilla a la que se le
pasa el archivo a convertir, y se obtiene 3 particiones con entrenamiento,
validación y test cada una.

\section{Optimización con algoritmos genéticos}

El módulo más importante de código está situado en
\texttt{src/ga\_optimization} y alberga el algoritmo genético que evolucionará
las configuraciones de perceptrones multicapa elegidas.

Empezando por la unidad más pequeña de medida de un individuo, el gen está
definido en el contexto de pesos y sesgos de cada neurona. Cada individuo
compuesto por capas ocultas mas la externa externa y sus configuraciones está
inicializado a partir de distribuciones uniformes con valores en el rango
$[-1.0, 1.0]$. En la creación se le indican las capas ocultas que va a tener,
si las capas ocultas van a ser modificables, cuántos datos de entrada tendrá la
red y cuántos datos de salida. A parte, evidentemente, del número de neuronas
de cada capa. Así, con una semilla definida anteriormente para permitir
reproducibilidad es como se construyen los individuos.

La definición de la función de evaluación es como sigue:

\begin{minted}{python}
def individual_evaluator(
    individual: MLPIndividual, trn: Proben1Split, tst: Proben1Split, **kwargs,
):
    """Evaluate an individual.

    :param individual: current individual to evaluate.
    :param trn: training data and labels.
    :param tst: validation data and labels.
    :param multi_class: ``True`` if the dataset is for multiclass
        classification.
    :returns: the fitness values.

    """
\end{minted}

Teniendo el individuo, el conjunto que va a ser usado como entrenamiento, el
conjunto con el que se van a probar los datos y si el problema es de
clasificación multiclase, se calculan las medidas que se comentaron en el
\autoref{chap:analysis}. La primera parte de la función de evaluación consiste
en construir un el modelo dada la configuración del individuo.

\begin{minted}{python}
    multi_class = kwargs.get("multi_class", False)
    start_time = time.perf_counter()
    units_size_list = [
        layer.config["units"] for layer in individual.layers[:-1]
    ]
    DGPLOGGER.debug(
        f"    Evaluating individual with neuron number: {units_size_list}"
    )
    # Create the model with the individual configuration
    model = Sequential()

    for layer_index, layer in enumerate(individual.layers):
        model.add(Dense.from_config(layer.config))
        model.layers[layer_index].set_weights([layer.weights, layer.bias])

    model.compile(
        optimizer=SGD(),
        loss=CategoricalCrossentropy()
        if multi_class
        else BinaryCrossentropy(),
    )

    model.fit(
        trn.X, trn.y_cat, epochs=100, batch_size=16, verbose=0,
    )
\end{minted}

Tras la creación del modelo se procede a obtener las medidas necesarias. Con
los datos predichos, se obtienen la puntuación F2-score y el porcentaje de
error obtenido. También se muestra un resumen por pantalla y el tiempo que ha
llevado calcular las puntuaciones.

\begin{minted}{python}
    # Predict the scores
    predicted_y = model.predict_classes(tst.X)
    f2_score = fbeta_score(
        tst.y,
        predicted_y,
        beta=2,
        average="micro" if multi_class else "binary",
    )
    error_perc = (
        1.0 - accuracy_score(tst.y, predicted_y, normalize=True)
    ) * 100
    neuron_layer_score = sum(units_size_list) * len(units_size_list) - 1
    DGPLOGGER.debug(
        f"        error%={error_perc:.2f}\n"
        f"        neuron/layer-score={neuron_layer_score:.2f}\n"
        f"        f2-score={f2_score:.5f}\n"
        f"        evaluation time={time.perf_counter() - start_time: .2f} sec"
    )

    return (error_perc, neuron_layer_score, f2_score)
\end{minted}

A continuación se muestra un ejemplo de uso definiendo un individuo y
evaluándolo para el problema ``cancer1'':

<<term=False, evaluate=True>>=
from src.dgp_logger import DGPLOGGER
from src.ga_optimizer.toolbox import individual_evaluator
from src.ga_optimizer.types import MLPIndividual
from src.utils import read_proben1_partition

DGPLOGGER.configure_dgp_logger("DEBUG")

dataset = read_proben1_partition("cancer1")
hidden_layers_sequence = [4]
constant_hidden_layers = False
individual = MLPIndividual(
    dataset.nin, hidden_layers_sequence, constant_hidden_layers, dataset.nout
)

results = individual_evaluator(
    individual, dataset.trn, dataset.tst, fit_train_prob=1.0, multi_class=False
)
@

Una vez configuradas las herramientas a utilizar (función de evaluación,
individuo, operaciones, función de adecuación...) y evaluada toda la población
inicial, se procede con el bucle central.

Se usa la puntuación conjunta ideal como centinela junto con el número de
generaciones. También existe una comprobación por si no se ha mejorado el
resultado en 10 generaciones, se para el algoritmo, porque posiblemente haya
llegado a un mínimo local. Tras las comprobaciones de parada se selecciona la
descendencia que será cruzada y mutada con la función de selección: obtener los
mejores individuos de la población para luego reemplazar los peores. Se usa la
mitad de la población como descendencia.

\begin{algorithm}
    \caption{Genetic Algorithm loop}\label{alg:ga}
    \begin{algorithmic}[1]
        \State \textit{evaluate initial populatialgoritmoon}
        \While{\textit{max fit is less than ideal and there are generations left}}
            \If{\textit{max fit hasn't improved in some generations}}
                \State \textit{Stop algorithm}
            \EndIf
            \State \textit{select the offspring from the best population individuals}
            \State \textit{apply crossover to the offspring}
            \State \textit{apply mutations to the offspring}
            \State \textit{replace the worst population individuals with the modified offspring}
        \EndWhile
    \end{algorithmic}
\end{algorithm}

La función de cruce ocurre si se dan dos condiciones: que se cumpla la
probabilidad de cruce entre dos individuos, y que los dos individuos tengan la
misma estructura\footnote{Mismo número de capas y neuronas en cada capa}.

\begin{minted}{python}
def crossover_operator(ind1: MLPIndividual, ind2: MLPIndividual):
    """Apply crossover betweent two individuals.

    This method will swap neurons with two random points from a random layer.
    The neurons associated bias and weights are swapped.

    :param ind1: the first individual.
    :param ind2: the second individual.
    :returns: a tuple with  the cross points and the crossed layer.

    """
    # Choose randomly the layer index to swap. If the hidden layers of any of
    # the two individuals are constant, swap neurons from the output layer
    # neuron in the output layer.
    layer_index = (
        len(ind1) - 1
        if ind1.constant_hidden_layers or ind2.constant_hidden_layers
        else random.randint(0, len(ind1) - 1)
    )
    cx_pts = random.sample(range(len(ind1.layers[layer_index].bias)), 2)

    (
        ind1.layers[layer_index].weights[:, cx_pts[0] : cx_pts[1]],
        ind2.layers[layer_index].weights[:, cx_pts[0] : cx_pts[1]],
    ) = (
        ind2.layers[layer_index].weights[:, cx_pts[0] : cx_pts[1]].copy(),
        ind1.layers[layer_index].weights[:, cx_pts[0] : cx_pts[1]].copy(),
    )
    (
        ind1.layers[layer_index].bias[cx_pts[0] : cx_pts[1]],
        ind2.layers[layer_index].bias[cx_pts[0] : cx_pts[1]],
    ) = (
        ind2.layers[layer_index].bias[cx_pts[0] : cx_pts[1]].copy(),
        ind1.layers[layer_index].bias[cx_pts[0] : cx_pts[1]].copy(),
    )

    return cx_pts, layer_index
\end{minted}

Obteniendo aleatoriamente la capa en la que se va a aplicar el intercambio y
seleccionando dos índices de neuronas en esa capa se procede a intercambiar
pesos y sesgos. En el siguiente código se ve una muestra del funcionamiento.
Cabe destacar que si alguno de los dos individuos tiene las capas ocultas
fijas, se selecciona la capa de salida para el cruce.

<<term=False, evaluate=True>>=
import random

from src.dgp_logger import DGPLOGGER
from src.ga_optimizer.toolbox import crossover_operator
from src.ga_optimizer.types import MLPIndividual
from src.utils import read_proben1_partition

DGPLOGGER.configure_dgp_logger("DEBUG")
random.seed(12345)

dataset = read_proben1_partition("cancer1")
const_hidden_layers = False
individual_0 = MLPIndividual(
    dataset.nin, [], const_hidden_layers, dataset.nout
)
individual_1 = MLPIndividual(
    dataset.nin, [], const_hidden_layers, dataset.nout
)

w_0_before = individual_0.layers[0].bias.copy()
w_1_before = individual_1.layers[0].bias.copy()
print("\nBefore:")
print("Individual 0")
print(individual_0.layers[0].bias)
print("Individual 1")
print(individual_1.layers[0].bias)

crossover_operator(individual_0, individual_1)

w_0_after = individual_0.layers[0].bias.copy()
w_1_after = individual_1.layers[0].bias.copy()
print("\nAfter:")
print("Individual 0")
print(individual_0.layers[0].bias)
print("Individual 1")
print(individual_1.layers[0].bias)

print("\nComparison:")
print("Individual 0 before_bias == after_bias")
print(w_0_before == w_0_after)
print("Individual 1 before_bias == after_bias")
print(w_1_before == w_1_after)
@

Se observa efectivamente que los sesgos de la neurona 1 de la capa de salida se
han intercambiado por los del otro individuo.

Continuando con las operaciones llegamos a las mutaciones. Existen 4 tipos de
mutaciones que se le aplican a los individuos:

\begin{itemize}

    \item Mutación de pesos: aplica ruido a un porcentaje de genes de los pesos
    de las capas del individuo.

    \item Mutación de sesgos: igual que el anterior, pero en vez de a los
    pesos, el ruido es aplicado a los genes de los sesgos de cada neurona.

    \item Mutación de neuronas: añade o elimina la última neurona de una capa
    oculta aleatoria.

    \item Mutación de capas: en este caso, añade o elimina una capa oculta al
    modelo. El número de neuronas y si se elimina o añade se calcula
    aleatoriamente.

\end{itemize}

Como la operación de mutación de pesos y sesgos es igual, hay que indicarle qué
atributo se quiere cambiar. Primero se obtienen las capas a las que se le va a
aplicar la mutación. Si nuestro individuo tiene las capas ocultas constantes,
solo aplicaremos mutación a la capa de salida. En caso contrario, se le
aplicará a todo.

\begin{minted}{python}
def weights_mutator(
    individual: MLPIndividual, attribute: str, gen_prob: float
) -> int:
    """Mutate some individual weights genes.

    For each layer weights or bias, obtain a random :class:`np.ndarray`(with
    values in the range ``[0.0 and 1.0]``) with the same shape as the selected
    attribute and mutate the genes that satisfy the ``gen_prob`` probability
    with a value in the range ``[-0.5, 0.5]``

    :param individual: individual to mutate.
    :param attribute: attribute to mutate. Must be either ``weights`` or
        ``bias``.
    :param gen_prob: probability of a gen to mutate.
    :returns: number of genes mutated.

    """
    mutated_genes = 0

    layer_list = (
        [individual.layers[-1]]
        if individual.constant_hidden_layers
        else individual.layers
    )

    for layer in layer_list:
        weights = getattr(layer, attribute)
        weights_shape = weights.shape

        mask = np.random.rand(*weights_shape) < gen_prob
        mutated_genes += np.count_nonzero(mask)
        mutations = np.random.uniform(-0.5, 0.5, weights_shape)
        mutations[~mask] = 0
        weights += mutations

    return mutated_genes
\end{minted}

En el siguiente ejemplo vemos en acción la mutación de genes:

<<term=False, evaluate=True>>=
import numpy as np

from src.ga_optimizer.toolbox import weights_mutator
from src.ga_optimizer.types import MLPIndividual
from src.utils import read_proben1_partition

np.random.seed(123)

dataset = read_proben1_partition("cancer1")
const_hidden_layers = False
individual = MLPIndividual(dataset.nin, [], const_hidden_layers, dataset.nout)
weights_before = individual.layers[0].weights.copy()

print("\nBefore mutation")
print(individual.layers[0].weights)

mutated_genes = weights_mutator(individual, "weights", 0.5)
weights_after = individual.layers[0].weights.copy()

print(f"\nAfter mutation of {mutated_genes} genes:")
print(individual.layers[0].weights)
print("\nComparison of weights difference:")
print(weights_after - weights_before)
@

La mutación de un porcentaje de genes en los pesos del individuo se ha
efectuado correctamente. El resto de genes, queda intacto (diferencia de 0).

En cuanto a la mutación de neuronas (añadido o eliminado de éstas en una capa
aleatoria), ocurre en dos fases: añadido/eliminado de neuronas de la capa
seleccionada y modificación del número de entradas de la siguiente capa. Como
comentábamos, se obtiene aleatoriamente qué capa vamos a modificar, y dentro de
la capa, si se va a añadir o eliminar una neurona:

\begin{minted}{python}
def neuron_mutator(individual: MLPIndividual) -> int:
    """Add/remove one neuron from a random hidden layer.

    Randomly choose whether to add or remove a neuron.

    :param individual: individual to mutate.
    :returns: whether the neuron was added or removed.

    """
    # We want to ignore output layer so it only adds/pops from a hidden layer
    layer_index = random.randint(0, len(individual) - 2)

    # Choose randomly to add or delete a neuron. If the number of neurons is
    # two, just add a new one.
    choice = (
        1
        if len(individual.layers[layer_index].bias) <= 2
        else random.choice((-1, 1))
    )
\end{minted}

Si se realiza la opción de añadir neurona, primero se modifican los pesos y
sesgos añadiendo un nuevo elemento, generado con una distribución uniforme en
el rango $[-0.5, 0.5]$. Después se añade la nueva entrada a la capa siguiente.

\begin{minted}{python}
    if choice > 0:
        # Get previous layer neurons as a reference for creating a new neuron
        # for this layer
        previous_layer_neurons = individual.layers[layer_index].weights.shape[
            0
        ]
        # Append a new neuron to the weights and bias of the chosen layer
        individual.layers[layer_index].weights = np.append(
            individual.layers[layer_index].weights,
            np.random.uniform(-0.5, 0.5, (previous_layer_neurons, 1)),
            axis=1,
        )
        individual.layers[layer_index].bias = np.append(
            individual.layers[layer_index].bias,
            [random.uniform(-0.5, 0.5)],
            axis=0,
        )
        # Append a new input entry for the chosen layer in the following layer
        next_layer_neurons = len(individual.layers[layer_index + 1].bias)
        individual.layers[layer_index + 1].weights = np.append(
            individual.layers[layer_index + 1].weights,
            np.random.uniform(-0.5, 0.5, (1, next_layer_neurons)),
            axis=0,
        )
\end{minted}

Si por el contrario vamos a eliminar la última neurona de la capa, se eliminan
las entradas de los pesos y sesgos de la capa elegida y se quita la entrada
correspondiente en la capa siguiente.

\begin{minted}{python}
    else:
        # Remove last neuron weights and bias from the chosen layer
        individual.layers[layer_index].weights = np.delete(
            individual.layers[layer_index].weights, -1, axis=1
        )
        individual.layers[layer_index].bias = np.delete(
            individual.layers[layer_index].bias, -1, axis=0
        )
        # Remove the input neuron from the next layer
        individual.layers[layer_index + 1].weights = np.delete(
            individual.layers[layer_index + 1].weights, -1, axis=0
        )
\end{minted}

Finalmente actualizamos las configuraciones de la capa elegida y la siguiente,
y devolvemos si el cambio ha sido de añadir o eliminar neurona.

\begin{minted}{python}
    # Update the units in the chosen and next layer config
    individual.layers[layer_index].config["units"] += choice
    individual.layers[layer_index + 1].config["batch_input_shape"][1] += choice

    return choice
\end{minted}

En el siguiente código vemos el añadido y la eliminación de neuronas:

<<term=False, evaluate=True>>=
import random

from src.ga_optimizer.toolbox import neuron_mutator
from src.ga_optimizer.types import MLPIndividual
from src.utils import read_proben1_partition

random.seed(12345)

dataset = read_proben1_partition("cancer1")
const_hidden_layers = False
individual = MLPIndividual(
    dataset.nin, [3], const_hidden_layers, dataset.nout
)

print("\nBefore first mutation. Weights and bias shapes:")
print(individual.layers[0].weights.shape, individual.layers[0].bias.shape)

neuron_change = neuron_mutator(individual)
result = "addition" if neuron_change > 0 else "subtraction"

print(f"\nAfter the {result} of one neuron. Weights and bias shapes:")
print(individual.layers[0].weights.shape, individual.layers[0].bias.shape)

neuron_change = neuron_mutator(individual)
result = "addition" if neuron_change > 0 else "subtraction"

print(f"\nAfter the {result} of one neuron. Weights and bias shapes:")
print(individual.layers[0].weights.shape, individual.layers[0].bias.shape)
@

Por último, existe la operación de mutar capas ocultas (eliminación y añadido).
Esta operación sólo puede modificar la última capa oculta del modelo, es decir,
añadir una nueva capa antes de la capa de salida, o eliminarla. Igual que se
hace en la mutación de neuronas primero se obtiene un número aleatorio en el
rango $[-1, 1]$ que decidirá si se añade o quita una capa. También se comprueba
si solo queda una capa, para entonces añadir obligatoriamente y no dar pie a
quitar esa única capa oculta.

\begin{minted}{python}
def layer_mutator(individual: MLPIndividual) -> int:
    """Add/remove one layer to the model.

    Compute whether to append a new hidden layer or pop the last one.

    :param individual: individual to mutate.
    :return: wether the layer was added or removed.

    """
    # Choose randomly to add or delete a layer. Ensure there are 2 or more
    # layers in the model before deleting one. The output layer is included in
    # the count.
    choice = 1 if len(individual) <= 2 else random.choice((-1, 1))

    difference = 0
\end{minted}

Lo que continúa se puede dividir en dos partes al igual que el operador de
neuronas. Primero se crea la capa nueva, con un número de neuronas aleatorio
dentro del rango $[2, 5]$. Esta información se usa para modificar la siguiente
capa: si por ejemplo teniendo la capa con forma $(9, 3)$ siendo $9$ las
neuronas de entrada de la capa y $3$ las propias de la capa, añadimos una capa
que tiene $5$ neuronas, la capa de salida tendría dos conexiones inexistentes.
Entonces hay ó que añadir neuronas de entrada (de forma similar a como se hace
en el operador de neuronas) en la siguiente capa, o eliminar las que quedan
huérfanas:

\begin{minted}{python}
    if choice > 0:
        # Choose a random number of neurons
        new_layer_output_neurons = random.randint(2, 5)
        # Obtain current last hidden layer neuron number
        previous_layer_output = individual.layers[-2].config["units"]
        # Insert a new hidden layer into the individual
        individual.append_hidden(
            Layer.uniform(
                name=f"Hidden{len(individual)}",
                input_neurons=previous_layer_output,
                output_neurons=new_layer_output_neurons,
            )
        )

        # Obtain the differences between the new layer neurons and the output
        # layer input neurons and apply necessary changes to this last one
        output_layer_input_neurons = individual.layers[-1].weights.shape[0]
        difference = new_layer_output_neurons - output_layer_input_neurons

        # Add input neuron entries
        if difference > 0:
            next_layer_neurons = len(individual.layers[-1].bias)
            individual.layers[-1].weights = np.append(
                individual.layers[-1].weights,
                np.random.uniform(-1.0, 1.0, (difference, next_layer_neurons)),
                axis=0,
            )
        # Remove input neuron entries
        elif difference < 0:
            individual.layers[-1].weights = np.delete(
                individual.layers[-1].weights,
                slice(
                    output_layer_input_neurons + difference,
                    output_layer_input_neurons,
                ),
                axis=0,
            )
\end{minted}

En segundo lugar, si por el contrario se ha decidido eliminar una capa hay que
obtener las neuronas de la capa anterior a la actual para después de eliminar
la última capa oculta, configurar correctamente la capa de salida:

\begin{minted}{python}
    else:
        # Obtain the predecessor output units and delte the chosen layer
        removed_predecessor_units = individual.layers[-3].config["units"]
        del individual.layers[-2]

        # Calculate the difference between the predecesor layer and the output
        # layer
        output_layer_input_len = individual.layers[-1].weights.shape[0]
        difference = removed_predecessor_units - output_layer_input_len

        # Append the neccesary input neuron entries
        if difference > 0:
            next_layer_neurons = len(individual.layers[-1].bias)
            individual.layers[-1].weights = np.append(
                individual.layers[-1].weights,
                np.random.uniform(-0.5, 0.5, (difference, next_layer_neurons)),
                axis=0,
            )
        # Remove the leftovers
        elif difference < 0:
            individual.layers[-1].weights = np.delete(
                individual.layers[-1].weights,
                slice(
                    output_layer_input_len + difference, output_layer_input_len
                ),
                axis=0,
            )
\end{minted}

Para finalizar, hay que modificar de nuevo la configuración de la capa de
salida y devolver el número de capas añadidas/eliminadas.

\begin{minted}{python}
    # Update output layer input neurons
    individual.layers[-1].config["batch_input_shape"][1] += difference

    return choice
\end{minted}

En el siguiente ejemplo se comprueba que efectivamente se añade y elimina la
última capa satisfactoriamente:

<<term=False, evaluate=True>>=
import random

from src.ga_optimizer.toolbox import layer_mutator
from src.ga_optimizer.types import MLPIndividual
from src.utils import read_proben1_partition

random.seed(12345)

dataset = read_proben1_partition("cancer1")
const_hidden_layers = False
individual = MLPIndividual(dataset.nin, [3], const_hidden_layers, dataset.nout)

print("\nBefore first mutation. The hidden layers:")
print([layer.config["units"] for layer in individual.layers[:-1]])

layer_change = layer_mutator(individual)
result = "addition" if layer_change > 0 else "subtraction"

print(f"\nAfter the {result} of one laye. The hidden layers:")
print([layer.config["units"] for layer in individual.layers[:-1]])

layer_change = layer_mutator(individual)
result = "addition" if layer_change > 0 else "subtraction"

print(f"\nAfter the {result} of one laye. The hidden layers:")
print([layer.config["units"] for layer in individual.layers[:-1]])
@

A parte de todo lo nombrado anteriormente que es lo más destacable del código,
existen varias funciones de utilidades, como la de mostrar un resumen de las
puntuaciones de los individuos después de cada generación, presentar un resumen
general al terminar el algoritmo, evaluar el mejor individuo de la generación
inicial y la final, ... Además también se pueden encontrar funciones de apoyo
que permiten por ejemplo, cuando se va a mutar la población calcular con un
número aleatorio si se van a mutar las neuronas o las capas. Lo mismo ocurre
con el cruce entre individuos. Todo el código esta liberado bajo la licencia
GPLv3\cite{gplv3} y se puede encontrar en GitHub\cite{deep-g-prop}.
