\chapter{Estado del arte} \label{chap:state-of-art}

En esta sección explicaré la elección de los \textit{frameworks} con los que
voy a trabajar, además de la comparativa con otros que no se adaptan las
necesidades del proyecto.

\section{Búsqueda de mejores parámetros} \label{sec:param-search}

La búsqueda de valores óptimos para los parámetros de entrada de las
\textit{redes neuronales artificiales} en cada problema puede llegar a requerir
demasiado tiempo y convertirse en una tarea tediosa y complicada. Por ello,
existen diferentes herramientas que facilitan esta búsqueda como son
\textit{Grid Search}\cite{sklearn-grid} y \textit{Random
Search}\cite{sklearn-random}.

A parte de estas herramientas bien conocidas, también se pueden usar otros
clasificadores ó algoritmos genéticos\cite{sklearn-deap} para la búsqueda de
parámetros. En esta sección se compararán los métodos anteriores con la
búsqueda utilizando algoritmos genéticos.

Las tres utilidades explicadas a continuación utilizan como entrada, entre
otros elementos, un conjunto de parámetros junto con los valores seleccionados
de éstos. Así, para cada parámetro le damos varias posibilidades que serán
utilizadas para entrenar la red neuronal y buscar la mejor puntuación.

\begin{table}[h]
    \centering
    \caption{Comparativa entre tres métodos de búsqueda de parámetros.}
    \label{tab:hp-optimization}
    \begin{tabular}{l|c|r}
        \textbf{Optimizator} & \textbf{Best Accuracy \%} &
            \textbf{Elapsed time (sec)} \\
        \hline
        Random Search  & 0.95103    &  119.76826    \\
        Grid Search    & 0.96271    & 6746.02930   \\
        Genetic Search & 0.93712    &  945.41645    \\
    \end{tabular}
\end{table}

Las pruebas realizadas que respaldan los datos de la
\autoref{tab:hp-optimization} se pueden encontrar en el directorio
\texttt{src/hp\_optimization/}. Para evaluar los optimizadores de parámetros se
ha utilizado como estimador \textit{MLPClassifier}\cite{sklearn-mlpclassifier}
de la librería \textit{scikit-learn}\cite{sklearn} y una validación cruzada de
4 divisiones. Se ha usado el conjunto de datos de \textit{digits} obtenido
también de \textit{scikit-learn}.

\subsection{Grid Search}

\textit{Grid search} es un método de búsqueda exhaustivo que, dada una función
de la que maximizar el resultado, compara todas las posibles combinaciones de
parámetros seleccionadas al inicio. Utilizando este método de búsqueda, tiene
asegurada la mejor combinación de parámetros posible, por contra, va a
necesitar demasiado tiempo (comparado con otras soluciones) para encontrarla.

\subsection{Random Search}

Similar a \textit{grid search}, éste algoritmo hace una búsqueda no tan
exhaustiva en la cual se eligen distintos valores para cada parámetro
utilizando un muestreo sobre éstos. En contraposición del método anterior, es
posible que no llegue a obtener los mejores parámetros pero mejora el tiempo
utilizado para la búsqueda.

\subsection{Evolutionary Search}

Ya se comentó sobre los algoritmos genéticos en el
\autoref{chap:problem-description}. Éstos pueden ser utilizados de manera
similar a las herramientas anteriores nombradas, acotando los cruces y
mutaciones a la tabla de parámetros definida.

%
% Framework selection
%

\section{\textit{Frameworks} para redes neuronales} \label{sec:framework-comp}

Dentro de este marco, partiendo de la inmensa cantidad de librerías que existen
para trabajar con redes neuronales, nos centraremos en las que permiten
ser usadas en Python. De éstas, destacaremos el subgrupo de las más
conocidas. Entre ellas están:

\begin{itemize}

    \item scikit-learn \cite{sklearn-nn}

    \item Keras\cite{keras-nn} (TensorFlow\cite{tensorflow-nn} and
        Theano\cite{theano-nn})

    \item PyTorch \cite{pytorch-nn} (Torch\cite{torch-nn})

\end{itemize}

Para la comparación entre los \textit{frameworks} nombrados, se ha utilizado
validación cruzada de 5 particiones y los mismos datos que en la sección
anterior: \autoref{sec:param-search}. Dado que los conjuntos de datos elegidos
son de ``juguete'', en el entrenamiento se podrá ver sobreajuste en todas las
medidas. Se han obtenido las siguientes puntuaciones para llevar la evaluación:

\begin{table}[h]
    \centering
    \caption{Comparación de tiempos de fit de los distintos frameworks de redes
        neuronales probados.}
    \label{tab:mlp-comp-times}
    \begin{tabular}{r|r}
        \mr{2}{*}{\textbf{Estimator}} & \textbf{Mean fit time} \\
                           & \textbf{(seconds)} \\
        \hline
        scikit-learn       &   8.82 \\
        Keras (Theano)     &  38.02 \\
        Keras (TensorFlow) &  95.62 \\
        skorch (Torch)     & 120.28 \\
    \end{tabular}
\end{table}

\begin{itemize}

    \item \textit{Accuracy} (\autoref{tab:mlp-comp-accuracy}). Consiste en el
    cálculo de los ejemplos bien clasificados. Un ejemplo bien clasificado
    sería por ejemplo el cual estando etiquetado como número 2, se clasifica
    como 2. La pega que tiene esta medida es que no considera los ejemplos mal
    clasificados, y eso puede conllevar perdida de información en conjuntos de
    datos desbalanceados.

    \item \textit{Precision}\cite{f-measure}
    (\autoref{tab:mlp-comp-precision}). Esta medida obtiene el porcentaje de
    ejemplos que no se han clasificado mal. A parte de los ejemplos bien
    clasificados como hace la métrica de \textit{accuracy}, ésta tiene en
    cuenta también los ejemplos clasificados como positivos que realmente no lo
    son.

    \item \textit{Recall}\cite{f-measure} (\autoref{tab:mlp-comp-recall}).
    Calcula la capacidad del clasificador de obtener todos los ejemplos
    positivos de una clase, así, tiene también en cuenta los ejemplos mal
    clasificados como de otra clase.

    \item \textit{F1-score}\cite{f-measure} (\autoref{tab:mlp-comp-f1-score}).
    En último lugar, aunque no menos importante, \textit{F1-score} o
    \textit{F1-measure} hace un balance entre \textit{precision} y
    \textit{recall} (media harmónica entre las dos medias).

\end{itemize}

A parte también se ha medido el tiempo total de ajuste del estimador. Esta va a
ser una medida crítica a la hora de elegir el \textit{framework} idóneo para el
trabajo, ya que la red neuronal se va re-ejecutar multiples veces con distintos
cambios.

Cabe destacar, que todas las medidas mostradas en las tablas de comparaciones
(\ref{tab:mlp-comp-times}, \ref{tab:mlp-comp-accuracy},
\ref{tab:mlp-comp-precision}, \ref{tab:mlp-comp-recall}, y
\ref{tab:mlp-comp-f1-score}) son la media de las 5 particiones utilizadas en la
validación cruzada. También se muestra la desviación estándar asociada a esa
lista de valores.

\begin{table}[]
    \centering
    \caption{Comparación de las puntuaciones de \textit{accuracy} de los
        distintos frameworks.}
    \label{tab:mlp-comp-accuracy}
    \begin{tabular}{r|c|c|c|c}
        \mr{2}{*}{\textbf{Estimator}} & \mc{2}{c|}{\textbf{Train accuracy}} & \mc{2}{c}{\textbf{Test accuracy}} \\
        \cline{2-5}
                           & \textbf{mean} & \textbf{std} & \textbf{mean} & \textbf{std} \\
        \hline
        scikit-learn       & 1.000000      & 0.000000     & 0.979958      & 0.008890 \\
        Keras (Theano)     & 1.000000      & 0.000000     & 0.982185      & 0.007401 \\
        Keras (TensorFlow) & 1.000000      & 0.000000     & 0.983859      & 0.003702 \\
        skorch (Torch)     & 1.000000      & 0.000000     & 0.977736      & 0.007483 \\
    \end{tabular}
\end{table}

\begin{table}[]
    \centering
    \caption{Comparación de las puntuaciones de \textit{precision} de los distintos frameworks.}
    \label{tab:mlp-comp-precision}
    \begin{tabular}{r|c|c|c|c}
        \mr{2}{*}{\textbf{Estimator}} & \mc{2}{c|}{\textbf{Train precision}} & \mc{2}{c}{\textbf{Test precision}}  \\
        \cline{2-5}
                           & \textbf{mean} & \textbf{std} & \textbf{mean} & \textbf{std} \\
        \hline
        scikit-learn       & 1.000000      & 0.000000     & 0.981268      & 0.008253 \\
        Keras (Theano)     & 1.000000      & 0.000000     & 0.982662      & 0.006576 \\
        Keras (TensorFlow) & 1.000000      & 0.000000     & 0.984674      & 0.003597 \\
        skorch (Torch)     & 1.000000      & 0.000000     & 0.977820      & 0.007539 \\
    \end{tabular}
\end{table}

\begin{table}[]
    \centering
    \caption{Comparación de las puntuaciones de \textit{recall} de los
        distintos frameworks.}
    \label{tab:mlp-comp-recall}
    \begin{tabular}{r|c|c|c|c}
        \mr{2}{*}{\textbf{Estimator}} & \mc{2}{c|}{\textbf{Train recall}} & \mc{2}{c}{\textbf{Test recall}} \\
        \cline{2-5}
                           & \textbf{mean} & \textbf{std} & \textbf{mean} & \textbf{std} \\
        \hline
        scikit-learn       & 1.000000      & 0.000000     & 0.979124      & 0.010237 \\
        Keras (Theano)     & 1.000000      & 0.000000     & 0.982052      & 0.007579 \\
        Keras (TensorFlow) & 1.000000      & 0.000000     & 0.984044      & 0.004134 \\
        skorch (Torch)     & 1.000000      & 0.000000     & 0.977400      & 0.008047 \\

    \end{tabular}
\end{table}

\begin{table}[]
    \centering
    \caption{Comparación de las puntuaciones de \textit{F1 score} de los
        distintos frameworks.}
    \label{tab:mlp-comp-f1-score}
    \begin{tabular}{r|c|c|c|c}
        \mr{2}{*}{\textbf{Estimator}} & \mc{2}{c|}{\textbf{Train F1 score}} & \mc{2}{c}{\textbf{Test F1 score}}  \\
        \cline{2-5}
                           & \textbf{mean} & \textbf{std} & \textbf{mean} & \textbf{std} \\
        \hline
        scikit-learn       & 1.000000      & 0.000000     & 0.979859      & 0.009372 \\
        Keras (Theano)     & 1.000000      & 0.000000     & 0.982067      & 0.007249 \\
        Keras (TensorFlow) & 1.000000      & 0.000000     & 0.984094      & 0.003992 \\
        skorch (Torch)     & 1.000000      & 0.000000     & 0.977304      & 0.007888 \\
    \end{tabular}
\end{table}

\subsection{scikit-learn}

scikit-learn es una colección de herramientas sobre aprendizaje automático,
proveyendo desde múltiples métodos de preprocesamiento, hasta estimadores de
todo tipo. Nosotros nos centraremos en la clase \textit{MLPClassifier} que
implementa una red neuronal simple o perceptrón multicapa. Como se observa en
la \autoref{tab:mlp-comp-times}, evalúa con el tiempo más competitivo de los
\textit{frameworks} utilizados. Éste estimador incluso obtiene de las mejores
puntuaciones en todos los campos estudiados, pero no tiene tan buen soporte
para trabajar con los pesos, activaciones y neuronas ocultas como Keras, por
ejemplo.

Es entendible que carezca de estas facilidades dado que no es un proyecto
centrado en los perceptrones multicapa exclusivamente, sino en cantidad de
diferentes estimadores y multitud de utilidades como se comentó al inicio.

\subsection{skorch}

A diferencia de la anterior, skorch si está centrada en el desarrollo de redes
neuronales artificiales. Su finalidad es proporcionar la facilidad de
scikit-learn con la flexibilidad de PyTorch. Consiste en una capa de
abstracción sobre PyTorch que expone las funciones más comunes de scikit-learn
mediante la clase \textit{NeuralNetClassifier}.

Este \textit{framework} queda descartado dado el gran tiempo de ajuste que
necesita el modelo para adaptarse a los datos de entrenamiento, siendo éste
algo importante a tener en cuenta. Por otro lado, éste sí permite interacción
sencilla con los pesos y las neuronas de la red.

\subsection{Keras}

Por ultimo y no menos importante se han hecho pruebas con la herramienta Keras.
Permite, una vez definido un código, ejecutar los modelos con distintos
\textit{backends} (como Theano, TensorFlow o MXNet\cite{mxnet-nn}) haciéndola
así una herramienta muy flexible. Planteada como una librería para la
experimentación rápida, goza de una comunidad gigante gracias a que está
respaldada por Google. A parte de la buena documentación, podemos observar en
las pruebas que los dos \textit{backends} utilizados consiguen las mejores
puntuaciones de todos los \textit{frameworks} comentados previamente. Así,
siendo Theano el \textit{backend} más rápido de los dos \textit{backends} será
el que use para este proyecto.

\subsection{DEAP}

DEAP\cite{deap-ga}, de sus iniciales: \textit{Distributed Evolutionary
Algorithms in Python}, es un \textit{framework} que permite desarrollar
algoritmos genéticos (entre muchas utilidades más) en Python. Existen otras
herramientas similares (como Platypus\cite{platypus-ga}) que no se ajustan al
problema tan bien como DEAP (o que cuya funcionalidad ya viene contenida en
DEAP). Por lo tanto, será el \textit{framework} que utilize para desarrollar el
algoritmo genético que entrene las redes neuronales.
